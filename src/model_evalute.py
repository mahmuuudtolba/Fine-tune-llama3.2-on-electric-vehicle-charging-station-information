import os
import json
from openai import OpenAI
from llama_cpp import Llama
import evaluate
from config.path_config import *
from transformers import AutoTokenizer
class QAEvaluator:
    """
    A class to evaluate QA performance of a quantized GGUF model via llama.cpp against a base HuggingFace model.
    """
    def __init__(self):
        # Load dataset
        self.dataset = [
                            {
                                "instruction": " what is the average annual rate at which an EV battery's capacity declines?",
                                "reference": "The text states that there is an average decline across all electric vehicles of around 2.3 percent per year."
                            },
                            {
                                "instruction": "What is the typical battery warranty offered by most EV manufacturers, in terms of years or distance?",
                                "reference": "Most EV manufacturers will give between 8-10 years warranty on their battery, or up to 100,000 km (62,000 miles)."
                            },
                            {
                                "instruction": "How does the projected lifespan of an EV battery compare to the average life expectancy of the car itself?",
                                "reference": "EV batteries have a projected lifespan of 15 to 20 years, which is longer than the average 12-year life expectancy of a car, meaning the battery will, in most cases, outlive the vehicle."
                            },
                            {
                                "instruction": "What is the key difference between a Mode 2 and a Mode 3 charging cable as described in the document?",
                                "reference": "A Mode 2 charging cable connects an EV to a standard household outlet, while a Mode 3 charging cable connects the vehicle to a dedicated EV charging station and is the most common for AC charging."
                            },
                            {
                                "instruction": "What is the specific use for Mode 4 charging cables and what special feature is mentioned to handle the heat?",
                                "reference": "Mode 4 charging cables are used for DC (level 3) fast charging and are often liquid-cooled to deal with the heat generated by the high power transfer."
                            },
                            {
                                "instruction": "Why does the text say DC fast charging is the most expensive option?",
                                "reference": "DC charging is the most expensive because the stations have high installation and operating costs needed to deliver large amounts of power (50 to 350 kW), and service providers pass these costs on to the customer."
                            },
                            {
                                "instruction": "How does the document suggest an EV owner can calculate the cost of charging their car at home?",
                                "reference": "To calculate home charging costs, you should take the price per kWh from your energy bill and multiply it by the size of your vehicle's battery."
                            },
                            {
                                "instruction": "What are the four common tariff structures that public charging providers use to calculate costs?",
                                "reference": "The four most common ways to calculate charging tariffs are: a connection fee, an energy fee (per kWh), a time fee (per minute/hour), and a service fee."
                            },
                            {
                                "instruction": "what two developments are making range anxiety an increasingly unwarranted concern?",
                                "reference": "The increase in the average range of EVs and the rapid development of charging infrastructure, especially DC fast charging, are making range anxiety less of a concern."
                            },
                            {
                                "instruction": "Why does the text argue that most EV drivers do not need to charge their car every night?",
                                "reference": "Because the average daily commute, such as roughly 62 km (39 miles) a day in the US, is significantly less than the maximum range of most modern EVs."
                            }
                        ]
                                          

        self.guff_model_path = MODEL_GUFF_PATH 
        self.base_model_id = 'meta-llama/Llama-3.2-3B-Instruct:novita'
        self.max_tokens = 128
        self.results_path = 'outputs/evaluation/evaluation_results.json'
        self.hf_client = OpenAI(
            base_url="https://router.huggingface.co/v1",
            api_key=os.environ.get('HF_TOKEN', '')
        )
        self.tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")


    def build_prompt(self,instruction):
        
        messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": {instruction}}
        ]

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        return prompt

    def evaluate_guff(self):
        llama = Llama(model_path=self.guff_model_path)
        rouge = evaluate.load('rouge')
        bleu = evaluate.load('bleu')
        preds, refs = [], []

        for item in self.dataset:
            prompt = self.build_prompt(item['instruction'])
            resp = llama(
                prompt=prompt,
                max_tokens=self.max_tokens,
                stop=["###"],
                echo=False
            )
            gen = resp['choices'][0]['text'].strip()

            preds.append(gen)
            refs.append(item['reference'])

        rouge_res = rouge.compute(predictions=preds, references=refs)
        bleu_res = bleu.compute(predictions=preds, references=refs)
        return {
            'rouge_1_f': rouge_res['rouge1'],
            'rouge_2_f': rouge_res['rouge2'],
            'rouge_l_f': rouge_res['rougeL'],
            'bleu_score': bleu_res['bleu']
        }


    def evaluate_base(self):
        rouge = evaluate.load('rouge')
        bleu = evaluate.load('bleu')
        preds, refs = [], []

        for item in self.dataset:
            resp = self.hf_client.chat.completions.create(
                model=self.base_model_id,
                messages=[{"role": "user", "content": item['instruction']}],
            )
            msg = resp.choices[0].message
            gen = msg.content.strip() if hasattr(msg, 'content') else str(msg).strip()
            preds.append(gen)
            refs.append(item['reference'])

        rouge_res = rouge.compute(predictions=preds, references=refs)
        bleu_res = bleu.compute(predictions=preds, references=refs)
        return {
            'rouge_1_f': rouge_res['rouge1'],
            'rouge_2_f': rouge_res['rouge2'],
            'rouge_l_f': rouge_res['rougeL'],
            'bleu_score': bleu_res['bleu']
        }


    def compute_improvements(self,base: dict, comp: dict):
        improvements = {}
        for key, base_val in base.items():
            comp_val = comp.get(key, 0.0)
            if base_val:
                improvements[key + '_improvement'] = (comp_val - base_val) / base_val * 100.0
            else:
                improvements[key + '_improvement'] = None
        return improvements

    def evalute(self):
        guff_metrics = self.evaluate_guff()
        base_metrics = self.evaluate_base()
        improvements = self.compute_improvements(base_metrics, guff_metrics)

        results = {
            'guff_model': self.guff_model_path,
            'base_model': self.base_model_id,
            'metrics': {
                'guff': guff_metrics,
                'base': base_metrics
            },
            'improvements_percent': improvements
        }
        with open(self.results_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"Evaluation complete. Results saved to {self.results_path}")
        return results

